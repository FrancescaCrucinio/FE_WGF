\documentclass[a4paper]{article}
\usepackage[margin = 3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{graphicx, subfig}

% bibliography
\usepackage[round]{natbib}

\DeclareMathOperator{\N}{\mathcal{N}}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\ent}{Ent}
\newcommand{\norm}[2]{\ensuremath{\Vert #1 \Vert_{#2}}}
\def\real{\mathbb{R}}

\title{Solving Fredholm integral equations of the first kind via Wasserstein gradient flow}
\author{Arnaud -> Adam, Francesca }
\date{ }

\begin{document}
\maketitle

\section{Fredholm integral equation of first kind}

We want to solve the integral equation
\[
\mu(y)=\int\rho\left(x\right)K(x,y)dx,
\]
where $\rho$ and $\mu$ are probability densities on $\real^{n}$ and $\real^{m}$ and $K$ a Markov transition density, i.e. $\mu=\rho K$ in operator notation. The solution to this problem is not unique and we propose to regularize the problem using an entropy constraint; i.e. for a given $\lambda>0$ we propose to minimize w.r.t. $\rho$
\begin{equation}
\label{eq:minimisation}
E(\rho)=\KL(\mu,\rho K)-\lambda\ent(\rho)
\end{equation}
where 
$\KL(\mu,\rho K)$ is the Kullback-Leibler divergence between $\mu$ and $\rho K$ and $\ent(\rho)=-\int\rho\log\rho$ is the entropy of $\rho$.
This requires solving a minimization problem in the space of probability measures. We are going to follow a Wasserstein gradient flow approach.

\section{Gradient flow approach}
\subsection{Notation}

Let us denote the set of probability measures with finite second moment on $\real^d$ by
\begin{align*}
\mathcal{P}_2(\real^d) = \left\lbrace \mu\in \mathcal{P}(\real^d): \int \norm{ x}{2}^2 \ d\mu(x)< \infty\right\rbrace
\end{align*}
and we define the 2-Wasserstein distance on this set
\begin{align*}
W_2(\mu, \nu) := \left( \inf_{\pi\in\Pi(\mu, \nu)}\int\norm{x - y}{2}\ d\pi(x, y)\right)^{1/2}
\end{align*}
where $\Pi(\mu, \nu)$ is the set of all possible couplings between $\mu$ and $\nu$.

For any proper and lower semicontinuous functional $F$ defined on $\mathcal{P}_2(\real^d)$ we define the sub-differential as the set of $\xi$ such that
\begin{align*}
F(\mu) - F(\nu) \geq \int\langle \xi(x), T(x) - x\rangle\ d\mu(x) +o\left( W_2(\mu, \nu)\right)
\end{align*}
with $T$ transport map from $\mu$ to $\nu$.

\subsection{Gradient flow} 

We consider the functional $E$ in \eqref{eq:minimisation} defined on $\mathcal{P}_2(\real^n)$. 
Following \citet[Definition 11.1.1]{ambrosio2008gradient}, we say that $\rho_t$ is a solution to the gradient flow equation for $E$ with the $W_2$ metric if
\begin{equation*}
v_t \in -\partial E(\rho_t)
\end{equation*}
where $\partial E(\rho_t)$ is the sub-differential of $E$ evaluated at $\rho_t$ and there exists $v_t$ is a "gradient" for $E$ at $\rho_t$ such that
\begin{align*}
\partial_{t}\rho_{t}=\nabla\cdot\left(\rho_{t}v_t\right),
\end{align*} 
holds.

To arrive to an expression for $v_t$ we compute the first variation of $E$
\begin{align*}
\lim_{\epsilon\rightarrow0}\epsilon^{-1}\left(E(\rho+\epsilon\chi)-E(\rho)\right)=\int\frac{\delta E}{\delta\rho}\left(x\right)\chi\left(dx\right),
\end{align*}
where $\chi$ is any signed measure such that $\rho+\epsilon\chi$ is a probability measure, and then we need to show that this is a sub-differential for $E$.

\subsubsection{First variation}

We have
\begin{align*}
E(\rho)= & \KL(\mu,\rho K)-\lambda\ent(\rho)\\
=- & \int\mu\log\rho K+\lambda\int\rho\log\rho+\int\mu\log\mu.
\end{align*}
It follows directly that 
\[
\frac{\delta\ent }{\left(\rho\right)}{\delta\rho}=1+\log\rho.
\]
and 
\begin{align*}
\int\mu\log\left(\left(\rho+\epsilon\chi\right)K\right)-\int\mu\log\left(\rho K\right) & =\int\mu\left\{ \log\left(\rho K\right)+\log\left(1+\frac{\epsilon\chi K}{\rho K}\right)\right\} -\int\mu\log\left(\rho K\right)\\
 & =\int\mu\log\left(1+\frac{\epsilon\chi K}{\rho K}\right)\\
 & =\int\mu\left(\frac{\epsilon\chi K}{\rho K}+o\left(\frac{\epsilon\chi K}{\rho K}\right)\right)\\
 & =\epsilon\int\mu\frac{\chi K}{\rho K}+o\left(\epsilon\int\mu\frac{\chi K}{\rho K}\right).
\end{align*}
We have 
\[
\int\mu\frac{\chi K}{\rho K}=\int\int\mu\left(dy\right)\frac{K(x,y)}{\rho K(y)}d\chi\left(x\right)
\]
so 
\[
\frac{\delta \KL}{\delta\rho}\left(x\right)=-\int\mu\left(dy\right)\frac{K(x,y)}{\rho K(y)}.
\]
Hence, it follows that 
\begin{equation}
\label{eq:derivative}
\frac{\delta E}{\delta\rho}\left(x\right)=-\int\mu\left(dy\right)\frac{K(x,y)}{\rho K(y)}+\lambda\left(1+\log\rho\left(x\right)\right).
\end{equation}
We can now compute the gradient of this functional derivative equation w.r.t. $x$
\[
\nabla\frac{\delta E}{\delta\rho}\left(x\right)=-\int\mu\left(dy\right)\frac{\nabla K(x,y)}{\rho K(y)}+\lambda\nabla\log\rho\left(x\right).
\]
We now consider the following PDE
\begin{equation}
\label{eq:pde}
\partial_{t}\rho_{t}=\nabla\cdot\left(\rho_{t}\nabla\frac{\delta E}{\delta\rho_{t}}\right),
\end{equation}
where $\nabla\cdot f=\sum_{i}\partial_{i}f_{i}$ is the divergence operator.
The corresponding nonlinear ODE 
\begin{equation}
dX_{t}=-\nabla\frac{\delta E}{\delta\rho_{t}}\left(X_{t}\right)dt,\quad X_{0}\sim\rho_{0}\label{eq:nonlinearODE}
\end{equation}
is such that $Law(X_{t})=\rho_{t}$.
Then, by construction, one has
\[
\frac{dE\left(\rho_{t}\right)}{dt}=-\int\left\Vert \nabla\frac{\delta E}{\delta\rho_{t}}\left(x\right)\right\Vert ^{2}\rho_{t}\left(dx\right).
\]
The terminology nonlinear ODE is here used to indicate that the drift depends not only on $X_{t}$ but on its distribution too.

Practically, what we would like to do is to simulate $N$ particles $(X_{t}^{1},...,X_{t}^{N})$ such that, at initialization, we sample iid particles  $X_{0}^{i}\sim\rho_{0}$ and then implement numerically the $N$ nonlinear ODEs
\[
dX_{t}^{i}=\int\mu\left(dy\right)\frac{\nabla K(X_{t}^{i},y)}{\rho_{t}^{N}K(y)}dt-\lambda\nabla\log\left(\rho_{t}^{N}*H_{\epsilon}\left(X_{t}^{i}\right)\right)dt,\quad\rho_{t}^{N}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_{t}^{i}}.
\]

Approximating the first term on the r.h.s. is fine as practically $\mu\left(dy\right)$ is a discrete measure but approximating $\nabla\log\rho_{t}\left(x\right)$ from the empirical measure $\rho_{t}^{N}$ is difficult and would require say convolution by some kernel $H_{\epsilon}$.
This is ugly and would be most likely highly inefficient.
In the next section, we show how to address this issue.

\subsection{Existence and uniqueness of the gradient flow}

Corollary 11.1.8 in \cite{ambrosio2008gradient} give existence of a gradient flow solution of \eqref{eq:pde} since the first variation \eqref{eq:derivative} is single-valued.
Uniqueness is given by Theorem 11.1.4 in \cite{ambrosio2008gradient} since the functional $E$ is geodesically convex in $\rho$:
\begin{align*}
E(\rho)= & \text{{KL}}(\mu,\rho K)-\lambda\text{{Ent}}(\rho)\\
=- & \int\mu\log\rho K+\lambda\int\rho\log\rho+\int\mu\log\mu.
\end{align*}
The last integral $\int\mu\log\mu$ is constant w.r.t. $\rho$ and the negative entropy $\lambda\int\rho\log\rho$ is geodesically convex in $\rho$ \citep[page 130]{santambrogio2017euclidean}.
Regarding the first integral $- \int\mu\log\rho K$, we have that
\begin{itemize}
\item $\rho K = \int K(x, y) \ d\rho(x)$ is geodesically convex if $K$ is convex in $x$ \citep[page 128]{santambrogio2017euclidean}
\item $-log s$ is a convex function
\end{itemize}
hence $\int\mu\left( -\log\rho K\right)$ is convex in $\rho$ (integrating w.r.t. $\mu$ does not change convexity).

\section{Nonlinear SDE approach and numerical implementation}

Let us rewrite the PDE 
\begin{align*}
\partial_{t}\rho_{t}= & \nabla\cdot\left(\rho_{t}\nabla\frac{\delta E}{\delta\rho_{t}}\right)\\
= & -\nabla\cdot\left(\rho_{t}\int\mu\left(dy\right)\frac{\nabla K(x,y)}{\rho_{t}K(y)}\right)+\lambda\nabla\cdot\left(\rho_{t}\nabla\log\rho_{t}\right).
\end{align*}
However, we have
\[
\nabla\cdot\left(\rho_{t}\nabla\log\rho_{t}\right)=\nabla\cdot\nabla\rho_{t}=\triangle\rho_{t},
\]
where $\triangle f=\sum_{i}\partial_{i}^{2}f_{i}$ is the Laplacian.
So we can consider the following non-linear SDE (McKean-Vlasov) 
\begin{equation}
dX_{t}=\int\mu\left(dy\right)\frac{\nabla K(X_{t},y)}{\rho_{t}K(y)}dt+\sqrt{2\lambda}dW_{t},\quad X_{0}\sim\rho_{0},\label{eq:nonlinearSDE}
\end{equation}
where $W_{t}$ is a standard $n-$dimensional Brownian motion.
The SDE \eqref{eq:nonlinearSDE} has the same marginal distributions as the nonlinear ODE \eqref{eq:nonlinearODE}.
So to solve the minimization problem of interest, we will simulate in practice $N$ particles $(X_{t}^{1},...,X_{t}^{N})$ such that, at initialization, we sample iid particles $X_{0}^{i}\sim\rho_{0}$ and then they evolve according to the non-linear (McKean-Vlasov) SDE
\[
dX_{t}^{i}=\int\mu\left(dy\right)\frac{\nabla K(X_{t},y)}{\rho_{t}^{N}K(y)}dt+\sqrt{2\lambda}dW_{t}^{i},\quad\rho_{t}^{N}=\frac{1}{N}\sum_{i=1}^{N}\delta_{X_{t}^{i}}.
\]

We will also need to further discretize in time these SDEs obviously.

\section{Connections with other methods}
Minimisation of a functional involving the $\KL$ divergence is a common method to solve Fredholm integral equations of the first kind.
For example, if we consider the functional
\begin{align*}
L(\rho) = \KL(\mu, \rho K) + \int \rho
\end{align*}
the corresponding first variation is
\begin{align*}
\frac{\delta L}{\delta \rho}= \int \mu\frac{K}{\rho K} - 1.
\end{align*}
Setting the first variation to 0 and multiplying by $\rho$ leads to the EM iteration
\begin{align*}
\rho\int \mu\frac{K}{\rho K} - \rho = 0
\end{align*}
\section{Toy Example}

We consider the toy Fredholm integral equation
\begin{equation*}
\N(y; m, \sigma_{\rho}^2 + \sigma_K^2) = \int \N(x; m, \sigma_{\rho}^2)\N(y; x, \sigma_K^2)\ dx,\qquad y\in\mathbb{R}.
\end{equation*}
Figure \ref{fig:at} shows the reconstruction of $\N(x; m, \sigma_{\rho}^2)$ with $m=0.5$, $\sigma_{\rho}^2 = 0.043^2$, $\sigma_K^2 = 0.045^2$.
We set $N=10^{4}$, $\lambda = 10$, $dt = 10^{-3}$. The initial distribution $\rho_0$ is uniform on $[0, 1]$.

\begin{figure}
\centering
\includegraphics[width = 0.8\textwidth]{analitically_tractable}
\caption{ }
\label{fig:at}
\end{figure}

\bibliographystyle{abbrvnat}
\bibliography{wgf_biblio}
\end{document}